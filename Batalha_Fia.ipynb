{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nimport pyspark.sql.functions as f\nimport re\nimport time"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["df = spark.createDataFrame([\n    ('id_cliente-1',  'cat-1, cat-2, cat-3'),\n    ('id_cliente-2',  'cat-1, cat-4, cat-5'),\n    ('id_cliente-3',  'cat-6, cat-7'),\n    ('id_cliente-4',  'cat-1, cat-2, cat-7, cat-10'),\n    ('id_cliente-5',  'cat-8, cat-10'),\n    ('id_cliente-6',  'cat-1, cat-9, cat-10'),\n    ('id_cliente-7',  'cat-1, cat-4, cat-5, cat-10'),\n    ('id_cliente-8',  'cat-7, cat-9'),\n    ('id_cliente-9',  'cat-1'),\n    ('id_cliente-10', 'cat-1, cat-2, cat-3, cat-4, cat-5, cat-6, cat-7, cat-8, cat-10')\n], ['id_cliente', 'categorias'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["df2 = spark.createDataFrame([\n    ('id_cliente-1',  'cat-1, cat-2, cat-3, cat-15'),\n    ('id_cliente-2',  'cat-1, cat-4, cat-5, cat-11, cat-14'),\n    ('id_cliente-3',  'cat-4, cat-14, cat-15'),\n    ('id_cliente-4',  'cat-1, cat-2, cat-7, cat-10'),\n    ('id_cliente-5',  'cat-8, cat-10, cat-11'),\n    ('id_cliente-6',  'cat-1, cat-9, cat-10, cat-11, cat-13'),\n    ('id_cliente-7',  'cat-1, cat-4, cat-5, cat-10'),\n    ('id_cliente-8',  'cat-7, cat-9, cat-12, cat-13, cat-14'),\n    ('id_cliente-9',  'cat-2'),\n    ('id_cliente-10', 'cat-1, cat-2, cat-3, cat-4, cat-5, cat-6, cat-7, cat-8, cat-10')\n], ['id_cliente', 'categorias'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["df1_ = spark.createDataFrame([''], ['id_cliente', 'cat-1', 'cat-2', 'cat-3', 'cat-4', 'cat-5', 'cat-6', 'cat-7', 'cat-8', 'cat-9', 'cat-10'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1163559722078741&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df1_ <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;id_cliente&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-1&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-2&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-3&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-4&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-5&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-6&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-7&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-8&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-9&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;cat-10&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    808</span>                 rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromRDD<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    809</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 810</span><span class=\"ansiyellow\">                 </span>rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromLocal<span class=\"ansiyellow\">(</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    811</span>             jrdd <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>SerDeUtil<span class=\"ansiyellow\">.</span>toJavaArray<span class=\"ansiyellow\">(</span>rdd<span class=\"ansiyellow\">.</span>_to_java_object_rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    812</span>             jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>applySchemaToPythonRDD<span class=\"ansiyellow\">(</span>jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_createFromLocal</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    440</span>         write temp files<span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    441</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 442</span><span class=\"ansiyellow\">         </span>data<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_wrap_data_schema<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    443</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>parallelize<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    444</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_wrap_data_schema</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    419</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    420</span>         <span class=\"ansigreen\">if</span> schema <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">None</span> <span class=\"ansigreen\">or</span> isinstance<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>list<span class=\"ansiyellow\">,</span> tuple<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 421</span><span class=\"ansiyellow\">             </span>struct <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_inferSchemaFromList<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">=</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    422</span>             converter <span class=\"ansiyellow\">=</span> _create_converter<span class=\"ansiyellow\">(</span>struct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    423</span>             data <span class=\"ansiyellow\">=</span> map<span class=\"ansiyellow\">(</span>converter<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_inferSchemaFromList</span><span class=\"ansiblue\">(self, data, names)</span>\n<span class=\"ansigreen\">    355</span>             warnings.warn(&quot;inferring schema from dict is deprecated,&quot;\n<span class=\"ansigreen\">    356</span>                           &quot;please use pyspark.sql.Row instead&quot;)\n<span class=\"ansigreen\">--&gt; 357</span><span class=\"ansiyellow\">         </span>schema <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>_merge_type<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>_infer_schema<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> row <span class=\"ansigreen\">in</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    358</span>         <span class=\"ansigreen\">if</span> _has_nulltype<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    359</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Some of types cannot be determined after inferring&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">&lt;genexpr&gt;</span><span class=\"ansiblue\">(.0)</span>\n<span class=\"ansigreen\">    355</span>             warnings.warn(&quot;inferring schema from dict is deprecated,&quot;\n<span class=\"ansigreen\">    356</span>                           &quot;please use pyspark.sql.Row instead&quot;)\n<span class=\"ansigreen\">--&gt; 357</span><span class=\"ansiyellow\">         </span>schema <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>_merge_type<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>_infer_schema<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> row <span class=\"ansigreen\">in</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    358</span>         <span class=\"ansigreen\">if</span> _has_nulltype<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    359</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Some of types cannot be determined after inferring&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansicyan\">_infer_schema</span><span class=\"ansiblue\">(row, names)</span>\n<span class=\"ansigreen\">   1060</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1061</span>     <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1062</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">raise</span> TypeError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Can not infer schema for type: %s&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1063</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1064</span>     fields <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>StructField<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">,</span> _infer_type<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansigreen\">True</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> items<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: Can not infer schema for type: &lt;class &apos;str&apos;&gt;</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df3 = df.select(\n        \"id_cliente\",\n        f.split(\"categorias\", \", \").alias(\"categorias\"),\n        f.posexplode(f.split(\"categorias\", \", \")).alias(\"pos\", \"val\")\n    )\\\n    .drop(\"val\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+--------------------+---+\n  id_cliente|          categorias|pos|\n+------------+--------------------+---+\nid_cliente-1|[cat-1, cat-2, ca...|  0|\nid_cliente-1|[cat-1, cat-2, ca...|  1|\nid_cliente-1|[cat-1, cat-2, ca...|  2|\nid_cliente-2|[cat-1, cat-4, ca...|  0|\nid_cliente-2|[cat-1, cat-4, ca...|  1|\nid_cliente-2|[cat-1, cat-4, ca...|  2|\nid_cliente-3|      [cat-6, cat-7]|  0|\nid_cliente-3|      [cat-6, cat-7]|  1|\nid_cliente-4|[cat-1, cat-2, ca...|  0|\nid_cliente-4|[cat-1, cat-2, ca...|  1|\nid_cliente-4|[cat-1, cat-2, ca...|  2|\nid_cliente-4|[cat-1, cat-2, ca...|  3|\nid_cliente-5|     [cat-8, cat-10]|  0|\nid_cliente-5|     [cat-8, cat-10]|  1|\nid_cliente-6|[cat-1, cat-9, ca...|  0|\nid_cliente-6|[cat-1, cat-9, ca...|  1|\nid_cliente-6|[cat-1, cat-9, ca...|  2|\nid_cliente-7|[cat-1, cat-4, ca...|  0|\nid_cliente-7|[cat-1, cat-4, ca...|  1|\nid_cliente-7|[cat-1, cat-4, ca...|  2|\n+------------+--------------------+---+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["def pegaNumero(x):\n  return(int(re.findall(r'\\d+', x)[0]))\n\ndef transforma(x):\n  return('client-uuid-'+str(x))\n\npegaNumeroUDF = udf(pegaNumero, IntegerType())\ntransformaUDF = udf(transforma, StringType())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["x1 = df.select('id_cliente', explode(split(\"categorias\",\",\")).alias(\"opa\")).withColumn(\"opa\", trim(col(\"opa\")))\nx1 = x1.withColumn('id', pegaNumeroUDF(\"id_cliente\"))\nx1 = x1.withColumn('opa_id', pegaNumeroUDF(\"opa\"))\nx1 = x1.groupBy('id_cliente','id').pivot('opa_id').count().na.fill(0).sort('id', ascending=True)\nx1 = x1.withColumn('id', transformaUDF(\"id\"))\nreplacements = {c:'cat-'+str(c) for c in x1.columns if c.isnumeric()}\nx1 = x1.select([col(c).alias(replacements.get(c, c)) for c in x1.columns])\nx1.drop('id_cliente').show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+\n            id|cat-1|cat-2|cat-3|cat-4|cat-5|cat-6|cat-7|cat-8|cat-9|cat-10|\n+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+\n client-uuid-1|    1|    1|    1|    0|    0|    0|    0|    0|    0|     0|\n client-uuid-2|    1|    0|    0|    1|    1|    0|    0|    0|    0|     0|\n client-uuid-3|    0|    0|    0|    0|    0|    1|    1|    0|    0|     0|\n client-uuid-4|    1|    1|    0|    0|    0|    0|    1|    0|    0|     1|\n client-uuid-5|    0|    0|    0|    0|    0|    0|    0|    1|    0|     1|\n client-uuid-6|    1|    0|    0|    0|    0|    0|    0|    0|    1|     1|\n client-uuid-7|    1|    0|    0|    1|    1|    0|    0|    0|    0|     1|\n client-uuid-8|    0|    0|    0|    0|    0|    0|    1|    0|    1|     0|\n client-uuid-9|    1|    0|    0|    0|    0|    0|    0|    0|    0|     0|\nclient-uuid-10|    1|    1|    1|    1|    1|    1|    1|    1|    0|     1|\n+--------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"Batalha_Fia","notebookId":814472296075044},"nbformat":4,"nbformat_minor":0}
